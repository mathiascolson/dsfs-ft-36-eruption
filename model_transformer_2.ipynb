{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2d4302",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deef02a",
   "metadata": {},
   "source": [
    "### Préparation des colonnes pré existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08aaeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import torch\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "file_path = \"pf_2020-03-30_filtered_downsampled.csv\"\n",
    "\n",
    "time_col = \"time\"        # nom de la colonne datetime dans le csv\n",
    "amp_col = \"amplitude\"           # si None, automatiquement détecté ou calculé\n",
    "fs = 1/60                   # 1/60 Hz (1 minute)\n",
    "win_min = 10         # window pour features (10 minutes)\n",
    "step_minuts = 10        # step (ici non chevauché). mettre < win_min pour chevauchement\n",
    "env_window = 200         # smoothing median window (200 valeurs)\n",
    "min_rows_for_env = env_window\n",
    "seq_length = 60       # longueur des séquences pour le modèle (en minutes)\n",
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Classe mapping (modifiable)\n",
    "# Par défaut : classes (0..4) :\n",
    "# 0 => delay > 24h\n",
    "# 1 => 16h < delay <= 24h\n",
    "# 2 => 1h < delay <= 12h\n",
    "# 3 => 0h < delay <= 1h\n",
    "# 4 => delay <= 0h (en cours)\n",
    "# NOTE: ceci est paramétrable ci-dessous\n",
    "thresholds_hours = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 1,\n",
    "    \"2\": 12,\n",
    "    \"3\": 16,\n",
    "    \"4\": 24\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Lecture, homogénéisation titre colonnes et colonnes temporelles\n",
    "# ---------------------------\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "# ---------------------------\n",
    "# Division de la colonne \"time\"\n",
    "# ---------------------------\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "if df[time_col].isna().any():\n",
    "    raise ValueError(\"Des valeurs non parsables dans la colonne time. Vérifier le format.\")\n",
    "\n",
    "# ajouter year/month/day/hour pour le modèle (toujours numériques)\n",
    "df[\"year\"]  = df[time_col].dt.year.astype(np.int16)\n",
    "df[\"month\"] = df[time_col].dt.month.astype(np.int8)\n",
    "df[\"day\"]   = df[time_col].dt.day.astype(np.int8)\n",
    "df[\"hour\"]  = df[time_col].dt.hour.astype(np.int8)\n",
    "df[\"minute\"]= df[time_col].dt.minute.astype(np.int8)\n",
    "df[\"seconde\"]= df[time_col].dt.second.astype(np.int8)\n",
    "\n",
    "\n",
    "start = pd.to_datetime('2020-04-02T08:20:00.000000Z') #Date de début de l'éruption\n",
    "end = pd.to_datetime('2020-04-06T09:30:00.000000Z') #Date de fin de l'éruption\n",
    "\n",
    "default_case = pd.NaT\n",
    "\n",
    "conditions = [\n",
    "    (df[time_col] > end),                           # 1. APRÈS l'intervalle\n",
    "    (df[time_col] >= start) & (df[time_col] <= end) # 2. PENDANT l'intervalle\n",
    "]\n",
    "choices = [\n",
    "    pd.NaT,          # 1. Si APRÈS -> NaN\n",
    "    pd.Timedelta(0)  # 2. Si PENDANT -> 0\n",
    "]\n",
    "\n",
    "df['delai_eruption'] = np.select(conditions, choices, default=pd.NaT)\n",
    "\n",
    "\n",
    "# suppression de la colonne time suite à sa division\n",
    "df.drop(\"time\", axis = 1)\n",
    "\n",
    "# ---------------------------\n",
    "#Extraction du type de composante : horizontale ou verticale\n",
    "# ---------------------------\n",
    "def type_component(channel):\n",
    "    c = str(channel).upper()\n",
    "\n",
    "    # EHZ, SHZ, BHZ, HHZ → verticale\n",
    "    if c.endswith(\"Z\"):\n",
    "        return \"vertical\"\n",
    "\n",
    "    # HHE, HHN, EHE, etc → horizontale\n",
    "    return \"horizontal\"\n",
    "\n",
    "df[\"component_type\"] = df[\"channel\"].apply(type_component)\n",
    "\n",
    "# Encodage numérique pour le modèle\n",
    "df[\"component_flag\"] = df[\"component_type\"].map({\n",
    "    \"horizontal\": 0,\n",
    "    \"vertical\": 1\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a8bd4",
   "metadata": {},
   "source": [
    "### Création et ajout des nouvelles features dans le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c3c1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Fonctions features\n",
    "# ---------------------------\n",
    "def shannon_entropy(segment, bins=50):\n",
    "    p, _ = np.histogram(segment, bins=bins, density=True)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def frequency_index_proxy(segment):\n",
    "    # proxy amplitude-based for fs=1Hz\n",
    "    med = np.median(np.abs(segment))\n",
    "    high = segment[np.abs(segment) > med]\n",
    "    low  = segment[np.abs(segment) <= med]\n",
    "    E_high = np.sum(high**2)\n",
    "    E_low  = np.sum(low**2)\n",
    "    if E_low == 0:\n",
    "        return np.nan\n",
    "    return float(E_high) / float(E_low)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Sliding windows: calcul des features\n",
    "#    WARNING: sur des millions de lignes, cette boucle est lente.\n",
    "#    Pour des datasets massifs, vectoriser ou utiliser numba/parallel est recommandé.\n",
    "# ---------------------------\n",
    "signal = df[amp_col].values.astype(float)\n",
    "n = len(signal)\n",
    "win = int(win_min)             # ex 60\n",
    "step = int(step_minuts)\n",
    "indices = range(0, n - win + 1, step)\n",
    "\n",
    "feat_list = []\n",
    "times_out = []\n",
    "\n",
    "for i in indices:\n",
    "    seg = signal[i:i+win]\n",
    "    t_center = df[time_col].iloc[i + win - 1]  # horodatage de fin de fenêtre (alignement comme étude)\n",
    "    SE = shannon_entropy(seg)\n",
    "    K  = float(kurtosis(seg, fisher=True, bias=False))\n",
    "    FI = frequency_index_proxy(seg)\n",
    "    std = float(np.std(seg))\n",
    "    mean = float(np.mean(seg))\n",
    "    med = float(np.median(seg))\n",
    "    per90 = float(np.percentile(seg,90))\n",
    "    per10 = float(np.percentile(seg,10))\n",
    "    tension = per90 - per10\n",
    "\n",
    "    feat_list.append((t_center, SE, K, FI, std, mean, med, per90, per10, tension))\n",
    "    times_out.append(t_center)\n",
    "\n",
    "# DataFrame features\n",
    "df_feat = pd.DataFrame(feat_list, columns=[\n",
    "    \"time\", \"SE\",\"Kurtosis\",\"FI\",\"std\",\"mean\",\"median\",\"per90\",\"per10\",\"tension\"\n",
    "])\n",
    "\n",
    "df_feat = df_feat.set_index(\"time\")\n",
    "\n",
    "# conserver la colonne time dans le dataset final (sous forme datetime index + une colonne time si nécessaire)\n",
    "df_feat[time_col] = df_feat.index\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Enveloppe median smoothing (200 valeurs)\n",
    "# ---------------------------\n",
    "if len(df_feat) < env_window:\n",
    "    # option : utiliser min_periods=1 pour avoir valeurs même si < env_window\n",
    "    df_feat[\"SE_env\"] = df_feat[\"SE\"].rolling(env_window, min_periods=1).median()\n",
    "    df_feat[\"FI_env\"] = df_feat[\"FI\"].rolling(env_window, min_periods=1).median()\n",
    "    df_feat[\"Kurt_env\"] = df_feat[\"Kurtosis\"].rolling(env_window, min_periods=1).median()\n",
    "    # idem pour autres stats si souhaité\n",
    "    df_feat[\"std_env\"] = df_feat[\"std\"].rolling(env_window, min_periods=1).median()\n",
    "else:\n",
    "    df_feat[\"SE_env\"] = df_feat[\"SE\"].rolling(env_window).median()\n",
    "    df_feat[\"FI_env\"] = df_feat[\"FI\"].rolling(env_window).median()\n",
    "    df_feat[\"Kurt_env\"] = df_feat[\"Kurtosis\"].rolling(env_window).median()\n",
    "    df_feat[\"std_env\"] = df_feat[\"std\"].rolling(env_window).median()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Joindre colonnes date/year/month/day/hour depuis la table d'origine\n",
    "#    (pour que modèle ait ces features temporelles)\n",
    "# ---------------------------\n",
    "# on fait un merge left-on time (index) pour récupérer year/month/day/hour\n",
    "orig_time_df = df[[time_col,\"year\",\"month\",\"day\",\"hour\",\"minute\"]].set_index(time_col)\n",
    "# aligner via nearest join with tolerance = step_seconds (facultatif)\n",
    "df_feat = df_feat.join(orig_time_df, how=\"left\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Création du label à partir de 'delai_eruption' si présente dans df original\n",
    "#    else lever erreur ou construire via table fournie\n",
    "# ---------------------------\n",
    "# On accepte différents formats : Timedelta / seconds / hours / NaN\n",
    "\n",
    "# Si df original contient 'delai_eruption' (Timedelta) aligned on original time,\n",
    "# il faut produire une colonne delai_eruption pour df_feat.\n",
    "# Approche : si original df contient 'delai_eruption', faire forward fill / resample:\n",
    "if \"delai_eruption\" in df.columns:\n",
    "    # aligner : prendre la valeur du delai à la fin de la fenêtre\n",
    "    # index df_feat is times at window end -> map from original df by exact match (may fail)\n",
    "    # faire nearest merge:\n",
    "    df_temp = df[[time_col,\"delai_eruption\"]].set_index(time_col)\n",
    "    df_feat = df_feat.join(df_temp, how=\"left\")\n",
    "    # si delai_eruption est Timedelta, le garder ; sinon convertir en secondes si numeric\n",
    "else:\n",
    "    raise ValueError(\"Aucune colonne 'delai_eruption' trouvée dans le CSV d'origine. Fournir la table d'éruptions ou la colonne delai_eruption.\")\n",
    "\n",
    "# Normaliser le format de delai_eruption : convertir tout en float seconds\n",
    "def delai_to_hours(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    # Timedelta\n",
    "    if isinstance(x, pd.Timedelta):\n",
    "        return x.total_seconds() / 3600.0\n",
    "    # numpy timedelta64\n",
    "    if np.issubdtype(type(x), np.timedelta64):\n",
    "        return pd.to_timedelta(x).total_seconds() / 3600.0\n",
    "    # string parsable as timedelta? try to parse numbers\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            # tenter parse ISO duration or \"HH:MM:SS\" etc.\n",
    "            td = pd.to_timedelta(x)\n",
    "            return td.total_seconds() / 3600.0\n",
    "        except Exception:\n",
    "            try:\n",
    "                return float(x) / 3600.0   # si x donné en secondes\n",
    "            except:\n",
    "                return np.nan\n",
    "    # numeric seconds or hours\n",
    "    if isinstance(x, (int, float, np.floating, np.integer)):\n",
    "        # supposer en secondes si valeur > 24*3600 sinon si déjà en heures? difficile => supposer secondes\n",
    "        # si < 1000 on considère c'est déjà en heures\n",
    "        if abs(x) > 1000:\n",
    "            return float(x) / 3600.0\n",
    "        else:\n",
    "            return float(x)\n",
    "    return np.nan\n",
    "\n",
    "df_feat[\"delai_hours\"] = df_feat[\"delai_eruption\"].apply(delai_to_hours)\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Mapping labels (paramétrable)\n",
    "# ---------------------------\n",
    "def label_from_delay_hours(h):\n",
    "    # h: float hours or nan\n",
    "    if pd.isna(h):\n",
    "        return 0   # class 0 = \"pas de risque (>24h)\" par défaut pour NaN\n",
    "    if h <= 0:\n",
    "        return 4   # en cours\n",
    "    if h <= 1:\n",
    "        return 3   # <1h\n",
    "    if h <= 12:\n",
    "        return 2   # <12h\n",
    "    if h <= 16:\n",
    "        return 1   # >16h class (intermédiaire)\n",
    "    # else h > 16 -> class 0 (>24h / no risk) ; ajuster si besoin\n",
    "    return 0\n",
    "\n",
    "df_feat[\"label\"] = df_feat[\"delai_hours\"].apply(label_from_delay_hours).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc1abc",
   "metadata": {},
   "source": [
    "### Encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323fd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 9. Features finales et encodage\n",
    "# ---------------------------\n",
    "# Colonnes numériques à garder\n",
    "numeric_features = [\"SE\",\"Kurtosis\",\"FI\",\"std\",\"mean\",\"median\",\"per90\",\"per10\",\"tension\",\n",
    "                    \"SE_env\",\"FI_env\",\"Kurt_env\",\"std_env\",\n",
    "                    \"year\",\"month\",\"day\",\"hour\",\"minute\"]\n",
    "\n",
    "# garder uniquement celles existantes\n",
    "numeric_features = [c for c in numeric_features if c in df_feat.columns]\n",
    "\n",
    "# Colonnes catégorielles à encoder (ex: station, channel, component)\n",
    "categorical_candidates = [c for c in df.columns if c.lower() in (\"station\",\"channel\",\"network\")]\n",
    "categorical_candidates = [c for c in categorical_candidates if c in df.columns]\n",
    "# également si present une colonne 'component' ou 'channel'\n",
    "if \"channel\" in df.columns:\n",
    "    categorical_candidates.append(\"channel\")\n",
    "categorical_candidates = list(dict.fromkeys(categorical_candidates))\n",
    "\n",
    "# Build preprocess pipeline\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_candidates)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Remplir NaN numériques par median avant scaler\n",
    "df_feat[numeric_features] = df_feat[numeric_features].fillna(df_feat[numeric_features].median())\n",
    "\n",
    "# Remplir catégoriques na par 'unk'\n",
    "for c in categorical_candidates:\n",
    "    df[c] = df[c].fillna(\"unk\")\n",
    "    # aligner au df_feat times: map nearest value from original df if needed\n",
    "    # simple approach: forward fill in original df then join earlier would have done it.\n",
    "\n",
    "# Fit transformer on entire dataset (ou training only, best practice: fit on train)\n",
    "X_num = df_feat[numeric_features].values\n",
    "X_cat = df.loc[df_feat.index, categorical_candidates].values if len(categorical_candidates)>0 else None\n",
    "\n",
    "if len(categorical_candidates)>0:\n",
    "    X_cat = df[categorical_candidates].loc[df_feat.index].fillna(\"unk\").values\n",
    "    X_combined = preprocessor.fit_transform(pd.concat([df_feat[numeric_features], df[categorical_candidates].loc[df_feat.index]], axis=1))\n",
    "else:\n",
    "    X_combined = numeric_transformer.fit_transform(X_num)\n",
    "\n",
    "# final feature names count\n",
    "n_features = X_combined.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8d20f",
   "metadata": {},
   "source": [
    "### Création des séquences pour le modèle transformer et subdivision en train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 10. Construction des sequences (sliding) pour le modèle\n",
    "# ---------------------------\n",
    "# On construit sequences non chevauchantes (ou chevauchantes selon step_sequence)\n",
    "step_seq = 1  # si 1 -> séquences glissantes à chaque point; si seq_length -> non chevauchées\n",
    "X = X_combined\n",
    "y = df_feat[\"label\"].values\n",
    "T = len(X)\n",
    "\n",
    "seqs = []\n",
    "labels = []\n",
    "times_seq = []\n",
    "\n",
    "for i in range(0, T - seq_length, step_seq):\n",
    "    seq = X[i:i+seq_length]\n",
    "    # label associé au temps de fin de séquence (alignement)\n",
    "    lab = y[i+seq_length-1]\n",
    "    seqs.append(seq)\n",
    "    labels.append(lab)\n",
    "    times_seq.append(df_feat.index[i+seq_length-1])\n",
    "\n",
    "X_seq = np.stack(seqs)            # shape (N_seq, seq_length, n_features)\n",
    "y_seq = np.array(labels)          # shape (N_seq,)\n",
    "\n",
    "# ---------------------------\n",
    "# 11. Train/Val/Test split temporel\n",
    "# ---------------------------\n",
    "N = len(X_seq)\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "test_frac = 0.15\n",
    "\n",
    "i_train_end = int(N * train_frac)\n",
    "i_val_end   = int(N * (train_frac + val_frac))\n",
    "\n",
    "X_train = X_seq[:i_train_end]\n",
    "y_train = y_seq[:i_train_end]\n",
    "X_val   = X_seq[i_train_end:i_val_end]\n",
    "y_val   = y_seq[i_train_end:i_val_end]\n",
    "X_test  = X_seq[i_val_end:]\n",
    "y_test  = y_seq[i_val_end:]\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_t   = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_t   = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64977f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
