{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15ba383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c851f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\F'\n",
      "C:\\Users\\Vartkirl\\AppData\\Local\\Temp\\ipykernel_22304\\660156240.py:4: SyntaxWarning: invalid escape sequence '\\F'\n",
      "  FILE_PATH = \"D:\\Formation_Data_Engineer\\Data_FullStack\\Data_Engineer_Full_Stack\\Projet_groupe\\pf_2020-03-30_filtered_downsampled.csv\"  # remplacer si besoin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built sequences: (401629, 60, 16) labels: (401629, 6)\n",
      "Split sizes (train/val/test): 281140 60244 60245\n",
      "Final shapes after encoding/scaling: (281140, 60, 19) (60244, 60, 19) (60245, 60, 19)\n",
      "Preprocessing complete. Loaders ready.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# CONFIG - à ajuster\n",
    "# -----------------------------\n",
    "FILE_PATH = \"D:\\Formation_Data_Engineer\\Data_FullStack\\Data_Engineer_Full_Stack\\Projet_groupe\\pf_2020-03-30_filtered_downsampled.csv\"  # remplacer si besoin\n",
    "time_col = \"time\"\n",
    "amp_col = \"amplitude\"\n",
    "chan_col = \"channel\"\n",
    "station_col = \"station\"\n",
    "\n",
    "# fenêtrage (exemples)\n",
    "fs = 1/60                # échantillonnage supposé (1/60 Hz données en minute)\n",
    "win_minut = 10        # taille fenêtre pour features en secondes (ex : 60 -> 1 min)\n",
    "step_min = 10       # pas entre fenêtres (ici pas chevauchement)\n",
    "env_window = 200        # smoothing median window (200 valeurs comme l'étude)\n",
    "\n",
    "# séquences (en nombre de fenêtres)\n",
    "seq_length = 60         # nombre de fenêtres par séquence\n",
    "step_seq = 1            # sliding step pour construire sequences\n",
    "\n",
    "# split temporel (appliqué après concat de toutes les sequences)\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "test_frac = 0.15\n",
    "\n",
    "# éruption (adapter / paramétrer par fichier si nécessaire)\n",
    "eruption_start = pd.to_datetime(\"2020-04-02T08:20:00Z\")\n",
    "eruption_end   = pd.to_datetime(\"2020-04-06T09:30:00Z\")\n",
    "\n",
    "# random seed reproducibility\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "torch.manual_seed(RND)\n",
    "\n",
    "eruption_start = eruption_start.tz_localize(None)\n",
    "eruption_end   = eruption_end.tz_localize(None)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fonctions utilitaires\n",
    "# -----------------------------\n",
    "def type_component(ch):\n",
    "    c = str(ch).upper()\n",
    "    return 0 if c.endswith(\"Z\") else 1  # 0 vertical, 1 horizontal\n",
    "\n",
    "def shannon_entropy(segment, bins=50):\n",
    "    p, _ = np.histogram(segment, bins=bins, density=True)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def compute_delay_hours(ts):\n",
    "    \"\"\"\n",
    "    Retourne le délai (en heures) avant le début de l'éruption (eruption_start).\n",
    "    - si ts > eruption_end -> np.nan (on ignore après la période)\n",
    "    - si ts in [eruption_start, eruption_end] -> 0.0\n",
    "    - else -> (eruption_start - ts) en heures\n",
    "    \"\"\"\n",
    "    if pd.isna(ts):\n",
    "        return np.nan\n",
    "    if ts > eruption_end:\n",
    "        return np.nan\n",
    "    if eruption_start <= ts <= eruption_end:\n",
    "        return 0.0\n",
    "    return (eruption_start - ts).total_seconds() / 3600.0\n",
    "\n",
    "def compute_delay_class(hours):\n",
    "\n",
    "    # si hours == np.nan -> renvoie vecteur de 0 (ou préférence: class 0?). ici retourne 0 vecteur (aucune classe positive)\n",
    "    if pd.isna(hours):\n",
    "        return np.array([1,0,0,0,0,0], dtype=int)  # garder \"classe 0\" par défaut pour NaN\n",
    "    conds = [\n",
    "        hours >= 24,\n",
    "        hours < 24,\n",
    "        hours < 16,\n",
    "        hours < 12,\n",
    "        hours < 1,\n",
    "        hours <= 0\n",
    "    ]\n",
    "    return np.array(conds, dtype=int)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Lecture et checks\n",
    "# -----------------------------\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# normaliser noms colonnes\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# parse time\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[time_col])\n",
    "df[time_col]   = df[time_col].dt.tz_localize(None)\n",
    "# ajouter component_flag (0 vertical, 1 horizontal) derived from channel\n",
    "df[\"component_flag\"] = df[chan_col].apply(type_component).astype(int)\n",
    "\n",
    "# ajouter colonnes temporelles utiles (optionnelles, seront récupérées par window-end)\n",
    "df[\"year\"] = df[time_col].dt.year.astype(int)\n",
    "df[\"month\"] = df[time_col].dt.month.astype(int)\n",
    "df[\"day\"] = df[time_col].dt.day.astype(int)\n",
    "df[\"hour\"] = df[time_col].dt.hour.astype(int)\n",
    "df[\"minute\"] = df[time_col].dt.minute.astype(int)\n",
    "df[\"seconde\"] = df[time_col].dt.second.astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Construction des features PAR COUPLE (station, channel)\n",
    "#    -> calcule features par fenêtre (alignées sur la fin de fenêtre),\n",
    "#       applique smoothing local, puis construit sequences (avant concat)\n",
    "# -----------------------------\n",
    "all_seqs = []        # sequences (seq_length, feat_dim)\n",
    "all_labels = []      # target vectors length 6 (np.int8)\n",
    "all_times = []       # timestamp of sequence end (for temporal split)\n",
    "all_stations = []    # station id (string) per sequence\n",
    "all_components = []  # component_flag per sequence\n",
    "\n",
    "win = int(win_minut)    # 1 sample/min\n",
    "step = int(step_min)\n",
    "\n",
    "grouped = df.groupby([station_col, chan_col])\n",
    "\n",
    "for (st, ch), g in grouped:\n",
    "    g = g.sort_values(time_col).reset_index(drop=True)\n",
    "    sig = g[amp_col].values.astype(float)\n",
    "    times = g[time_col].values\n",
    "    n = len(sig)\n",
    "    if n < win:\n",
    "        continue\n",
    "\n",
    "    # features per window\n",
    "    feat_rows = []\n",
    "    feat_times = []\n",
    "    for i in range(0, n - win + 1, step):\n",
    "        seg = sig[i:i+win]\n",
    "        t_end = pd.to_datetime(times[i + win - 1])  # window end timestamp (exists in g)\n",
    "        SE = shannon_entropy(seg)\n",
    "        K  = float(kurtosis(seg, fisher=True, bias=False))\n",
    "        std = float(np.std(seg))\n",
    "        mean = float(np.mean(seg))\n",
    "        med = float(np.median(seg))\n",
    "        p90 = float(np.percentile(seg, 90))\n",
    "        p10 = float(np.percentile(seg, 10))\n",
    "        tens = p90 - p10\n",
    "        feat_rows.append([SE, K, std, mean, med, p90, p10, tens])\n",
    "        feat_times.append(t_end)\n",
    "\n",
    "    feat_df = pd.DataFrame(feat_rows, columns=[\n",
    "        \"SE\",\"Kurtosis\",\"std\",\"mean\",\"median\",\"per90\",\"per10\",\"tension\"\n",
    "    ])\n",
    "    feat_df[\"time\"] = pd.to_datetime(feat_times)\n",
    "\n",
    "    # local smoothing (median envelope) per this channel/station\n",
    "    for col in [\"SE\",\"Kurtosis\",\"std\",\"mean\",\"median\",\"per90\",\"per10\",\"tension\"]:\n",
    "        if col in feat_df.columns:\n",
    "            feat_df[col + \"_env\"] = feat_df[col].rolling(window=env_window, min_periods=1).median()\n",
    "\n",
    "    # copy meta info (station, component flag) into feat_df rows\n",
    "    feat_df[\"station\"] = st\n",
    "    # component_flag extracted from original group (all rows same channel)\n",
    "    comp_flag = int(g[\"component_flag\"].iloc[0])\n",
    "    feat_df[\"component_flag\"] = comp_flag\n",
    "\n",
    "    # attach time-derived columns from original g by matching the window-end timestamps\n",
    "    tmp = g[[time_col, \"year\", \"month\", \"day\", \"hour\", \"minute\",\"seconde\"]].copy()\n",
    "    tmp = tmp.rename(columns={time_col: \"time\"})\n",
    "    feat_df = pd.concat([feat_df,tmp])\n",
    "\n",
    "    # build sequences (SLIDING) from feat_df BEFORE concat with other groups\n",
    "    M = len(feat_df)\n",
    "    if M < seq_length:\n",
    "        continue\n",
    "\n",
    "    # choose per-step features (take only existing columns)\n",
    "    per_step_features = [\n",
    "        \"SE\",\"Kurtosis\",\"std\",\"mean\",\"median\",\"per90\",\"per10\",\"tension\",\n",
    "        \"SE_env\",\"Kurtosis_env\",\"std_env\",\"mean_env\",\"median_env\",\"per90_env\",\"per10_env\",\"tension_env\"\n",
    "    ]\n",
    "    per_step_features = [c for c in per_step_features if c in feat_df.columns]\n",
    "\n",
    "    per_step_arr = feat_df[per_step_features].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # for each possible sequence in this channel/station\n",
    "    for i in range(0, M - seq_length + 1, step_seq):\n",
    "        seq = per_step_arr[i:i+seq_length]  # (seq_length, feat_dim)\n",
    "        t_seq_end = feat_df[\"time\"].iloc[i + seq_length - 1]\n",
    "        delay_h = compute_delay_hours(t_seq_end)\n",
    "        label_vec = compute_delay_class(delay_h)  # vector length 6, dtype int\n",
    "        all_seqs.append(seq)\n",
    "        all_labels.append(label_vec)\n",
    "        all_times.append(pd.to_datetime(t_seq_end))\n",
    "        all_stations.append(st)\n",
    "        all_components.append(comp_flag)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Concat all sequences across all station/channel groups\n",
    "#    Now sequences built independently per group are concatenated\n",
    "# -----------------------------\n",
    "if len(all_seqs) == 0:\n",
    "    raise RuntimeError(\"Aucune séquence construite. Vérifier fenêtrage / données.\")\n",
    "\n",
    "X = np.stack(all_seqs).astype(np.float32)      # (N_seq, seq_length, feat_dim)\n",
    "y = np.stack(all_labels).astype(np.int64)      # (N_seq, 6)\n",
    "times_arr = np.array(all_times)                # datetime64 array\n",
    "stations_arr = np.array(all_stations)          # (N_seq,)\n",
    "components_arr = np.array(all_components).astype(np.int64)  # (N_seq,)\n",
    "\n",
    "print(\"Built sequences:\", X.shape, \"labels:\", y.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Temporal split (train/val/test) BEFORE any scaling/encoding\n",
    "# -----------------------------\n",
    "order = np.argsort(times_arr)\n",
    "X = X[order]\n",
    "y = y[order]\n",
    "times_arr = times_arr[order]\n",
    "stations_arr = stations_arr[order]\n",
    "components_arr = components_arr[order]\n",
    "\n",
    "N = len(X)\n",
    "i_train = int(N * train_frac)\n",
    "i_val = int(N * (train_frac + val_frac))\n",
    "\n",
    "X_train = X[:i_train]; y_train = y[:i_train]; st_train = stations_arr[:i_train]; comp_train = components_arr[:i_train]\n",
    "X_val   = X[i_train:i_val]; y_val = y[i_train:i_val]; st_val = stations_arr[i_train:i_val]; comp_val = components_arr[i_train:i_val]\n",
    "X_test  = X[i_val:]; y_test = y[i_val:]; st_test = stations_arr[i_val:]; comp_test = components_arr[i_val:]\n",
    "\n",
    "print(\"Split sizes (train/val/test):\", len(X_train), len(X_val), len(X_test))\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Fit encoders/scalers on TRAIN only, then transform val/test\n",
    "#    Station OneHotEncoder fitted on station strings of train set\n",
    "#    Scaler fitted on flattened per-step numeric features of train set\n",
    "# -----------------------------\n",
    "\n",
    "# station OHE\n",
    "ohe = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", sparse_output=False)\n",
    "ohe.fit(np.array(st_train).reshape(-1,1))\n",
    "\n",
    "# scaler fitted on flattened train numeric features\n",
    "\n",
    "# flatten train for scaler: (N_train*L, F)\n",
    "Ntr, L, F = X_train.shape\n",
    "X_train_flat = X_train.reshape(Ntr * L, F)\n",
    "scaler = StandardScaler().fit(X_train_flat)\n",
    "\n",
    "def transform_and_concat(X_part, stations_part, components_part, scaler, ohe):\n",
    "\n",
    "    Np, Lp, Fp = X_part.shape\n",
    "    X_flat = X_part.reshape(-1, Fp)\n",
    "    X_flat = scaler.transform(X_flat)\n",
    "    X_scaled = X_flat.reshape(Np, Lp, Fp)\n",
    "\n",
    "    st_ohe = ohe.transform(np.array(stations_part).reshape(-1,1))  # (N, S)\n",
    "    comp_col = np.array(components_part).reshape(-1,1)            # (N, 1)\n",
    "\n",
    "    st_rep = np.repeat(st_ohe[:, np.newaxis, :], Lp, axis=1)      # (N, L, S)\n",
    "    comp_rep = np.repeat(comp_col[:, np.newaxis, :], Lp, axis=1)  # (N, L, 1)\n",
    "\n",
    "    X_out = np.concatenate([X_scaled, comp_rep.astype(np.float32), st_rep.astype(np.float32)], axis=2)\n",
    "    return X_out\n",
    "\n",
    "X_train_proc = transform_and_concat(X_train, st_train, comp_train, scaler, ohe)\n",
    "X_val_proc   = transform_and_concat(X_val,   st_val,   comp_val,   scaler, ohe)\n",
    "X_test_proc  = transform_and_concat(X_test,  st_test,  comp_test,  scaler, ohe)\n",
    "\n",
    "print(\"Final shapes after encoding/scaling:\", X_train_proc.shape, X_val_proc.shape, X_test_proc.shape)\n",
    "# y already shape (N,6)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Save arrays and build DataLoaders\n",
    "# -----------------------------\n",
    "np.save(\"X_train.npy\", X_train_proc)\n",
    "np.save(\"X_val.npy\", X_val_proc)\n",
    "np.save(\"X_test.npy\", X_test_proc)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "# Build torch loaders (labels as float for multi-label BCE or long for multiclass depending model)\n",
    "batch_size = 64\n",
    "# here labels are vectors (N,6) -> use float32 if training with BCEWithLogitsLoss, else adapt\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_proc).float(), torch.from_numpy(y_train).float())\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_val_proc).float(),   torch.from_numpy(y_val).float())\n",
    "test_ds  = TensorDataset(torch.from_numpy(X_test_proc).float(),  torch.from_numpy(y_test).float())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Preprocessing complete. Loaders ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a6b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
